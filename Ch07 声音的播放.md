<font size=3>**文档目录**</font>

[TOC]

# 第七节 声音的播放

> Update 2019/01/24 20:15

> Date: 2019/01/24

> Editor: NEU - Amy Li

> Contact: hsrlhl@outlook.com



## 1 &nbsp;概要

上一节，我们了解并熟悉了为声音的播放做准备工作的 Buffer 类，介绍了生产者 - 消费者模型，是整个项目的重要“后勤部队”。

在本节，我们真正完成声音的播放任务，它相当于项目中在前方冲锋陷阵的“士兵”，是我们统揽整个项目最直观的感受，也是整个项目方案中离底层最接近的部分。



## 2 &nbsp;音频采集播放基础知识补充

在 Andriod 设备上进行音频的采集和播放，需要掌握一些音频的相关知识，采样率、采样周期等概念我们之前已经整理过，除此之外，我们还需要了解音频通道、采样位数、PCM 和 BufferSizeBytes 等知识。

**（1）音频通道**

Andriod 设备支持**双声道立体声**和**单声道**两种。单声道的声音只能使用一个喇叭发声（有的也处理成两个喇叭输出同一个声道的声音），双声道立体声的 PCM 可以使两个喇叭都发声（一般左右声道有分工） ，更能感受到空间效果。CHANNEL_IN_MONO 是单声道，在所有设备上均支持，但 CHANNEL_IN_STEREO 是双声道立体声，并不是所有的设备均支持。

**（2）采样位数**

[采样位数](https://baike.baidu.com/item/%E9%87%87%E6%A0%B7%E4%BD%8D%E6%95%B0)，即采样值或取样值（就是将采样样本幅度量化）。它是用来衡量声音波动变化的一个参数，也可以说是声卡的分辨率。它的数值越大，分辨率也就越高，所发出声音的能力越强。

在计算机中采样位数一般有8位和16位之分，但有一点请大家注意，8位不是说把纵坐标分成8份，而是分成2的8次方即256份； 同理16位是把纵坐标分成2的16次方65536份。

**（3）PCM**

[PCM](https://baike.baidu.com/item/PCM/1568054?fr=aladdin)是英文Pulse-code modulation的缩写，中文译名是**脉冲编码调制**。

我们知道，在现实生活中，人耳听到的声音是模拟信号，PCM就是要把声音从模拟转换成数字信号的一种技术，也就是把一个时间连续，取值连续的[模拟信号](https://baike.baidu.com/item/%E6%A8%A1%E6%8B%9F%E4%BF%A1%E5%8F%B7)变换成时间离散，取值离散的数字信号后在[信道](https://baike.baidu.com/item/%E4%BF%A1%E9%81%93)中传输。脉冲编码调制是一个对模拟信号先**抽样**，再对样值幅度**量化**，最后进行**编码**的过程。

PCM 的原理简单地说就是利用一个固定的频率对模拟信号进行采样，采样后的信号在波形上看就像一串连续的幅值不一的脉冲，把这些脉冲的幅值按一定的精度进行量化，这些量化后的数值被连续地输出、传输、处理或记录到存储介质中，所有这些组成了数字音频的产生过程。Android 支持的采样大小为16bit 或者8bit。当然采样大小越大，那么信息量越多，音质也越高，现在主流的采样大小都是16bit，在低质量的语音传输的时候8bit足够了。

> [抽样](https://baike.baidu.com/item/%E6%8A%BD%E6%A0%B7)，就是对模拟信号进行周期性扫描，把时间上连续的信号变成时间上离散的信号，抽样必须遵循奈奎斯特抽样定理。该模拟信号经过抽样后还应当包含原信号中所有信息，也就是说能无失真的恢复原模拟信号。它的抽样速率的下限是由抽样定理确定的。
>
> 量化，就是把经过抽样得到的瞬时值将其幅度离散，即用一组规定的[电平](https://baike.baidu.com/item/%E7%94%B5%E5%B9%B3)，把瞬时抽样值用最接近的电平值来表示,通常是用二进制表示。
>
> 量化误差：量化后的信号和抽样信号的[差值](https://baike.baidu.com/item/%E5%B7%AE%E5%80%BC)。量化误差在接收端表现为噪声，称为量化噪声。 量化级数越多误差越小，相应的二进制码位数越多，要求传输速率越高，频带越宽。 为使量化噪声尽可能小而所需码位数又不太多，通常采用非均匀量化的方法进行量化。 非均匀量化根据幅度的不同区间来确定量化间隔，幅度小的区间量化间隔取得小，幅度大的区间量化间隔取得大。
>
> 一个模拟信号经过抽样量化后，得到已量化的脉冲幅度调制信号，它仅为有限个数值。
>
> 编码，就是用一组二进制码组来表示每一个有固定电平的量化值。然而，实际上量化是在编码过程中同时完成的，故编码过程也称为[模/数变换](https://baike.baidu.com/item/%E6%A8%A1%2F%E6%95%B0%E5%8F%98%E6%8D%A2)，可记作A/D。

**PCM 信号的两个重要指标是采样频率和量化精度。**目前，CD 音频的采样频率通常为 44100Hz，量化精度是16bit。通常，播放音频时，应用程序从存储介质中读取音频数据（MP3、WMA、AAC等），经过解码后，最终送到[音频驱动](https://www.baidu.com/s?wd=%E9%9F%B3%E9%A2%91%E9%A9%B1%E5%8A%A8&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)程序中的就是 PCM 数据；反过来，在录音时，音频驱动不停地把采样所得的 PCM 数据送回给应用程序，由应用程序完成压缩、存储等任务。所以，音频驱动的**两大核心任务**就是：

- **playback**——如何把用户空间的应用程序发过来的PCM数据，转化为人耳可以辨别的模拟音频
- **capture**——把麦克风接收到得模拟信号，经过采样、量化，转换为 PCM 信号送回给用户空间的应用程序

**（4）BufferSizeBytes**

配置完音频播放的音频源等基本参数，需要设置缓冲区大小。麦克风采集到的音频数据先放置在一个缓冲区里面，之后我们再从这个缓冲区里面读取数据，从而获取到麦克风录制的音频数据。在 Android 中不同的声道数、采样位数和采样频率会有不同的最小缓冲区大小，当传入的缓冲区大小小于最小缓冲区大小的时候则会初始化失败。大的缓冲区大小可以打开更为平滑的录制效果，相应的也会带来更大一点的延时。如下图所示展示了缓冲区在音频录制和获取中所处的位置：

<div align=center><img src="https://github.com/AmyLi98/Acoustic-Communication/blob/master/SinVoice1/doc/Tutorials/img/%E7%BC%93%E5%86%B2%E5%8C%BA%E5%A4%A7%E5%B0%8F.jpg" alt="图1 缓冲区" /></div>

这里提到的最小缓冲区的大小可以由 getMinBufferSize 方法获得，当获取最小缓冲区大小失败的时候，会返回相应的负数错误码。



## 3 &nbsp;MediaPlayer和AudioTrack的区别与联系

高频声波隐蔽信道通信在发送和接收信号时，需要分别调用声卡借助于扬声器和麦克风来播放和采集音频数据信息，在 Andriod 平台下处理媒体文件的 API 位于 andriod.media 包，其中的 MediaRecorder 和 AudioRecorder 均可以采集音频信息，MediaPlayer 和 AudioTrack 也都可以播放音频信息。

播放声音可以用 MediaPlayer 和 AudioTrack，两者都提供了 Java API 供应用开发者使用。虽然都可以播放声音，但两者还是有很大的区别的。

- **MediaPlayer：**可以同时处理视频和语音信息，可以播放多种格式的声音文件，例如MP3，AAC，WAV，OGG，MIDI等，使用时需要制定输入/输出文件。不适用于实时处理音频数据的场景。同时，MediaPlayer 会在 framework 层创建对应的音频解码器。

- **AudioTrack：**只能够处理音频信息，且只能够播放已经解码的 PCM 流，只支持 wav 格式的音频文件，因为 wav 格式的音频文件大部分都是 PCM 流。可用于实施产生音频数据的场景。AudioTrack 不创建解码器，所以只能播放不需要解码的 wav 文件。

当然两者之间还是有紧密的联系的，MediaPlayer 在 framework 层还是会创建 AudioTrack，把解码后的 PCM 数据流传递给 AudioTrack，AudioTrack 再传递给 AudioFlinger 进行混音，然后才传递给硬件播放。所以，MediaPlayer 包含了 AudioTRack。



## 4 &nbsp;利用 AudioTrack 播放声音

AudioTrack 是管理和播放单一音频资源的类，仅能播放已经解码的 PCM 流，用于 PCM 音频流的回放。

利用 AudioTrack 实现 PCM 音频的播放遵循以下**五个步骤：**

- 获取最小缓冲区大小

  若不设置缓冲区大小，默认计算缓冲区大小的函数如下：

  static public int getMinBufferSize(int sampleRateInHz, int channelConfig, int audioFormat);

  getMinBufferSize 函数的第一个参数为音频采样率、第二个参数为声道数、第三个参数表示每个采样点的位数。示例如下：

  ```java
  int bufferSize = AudioTrack.getMinBufferSize(16000,AudioFormat.CHANNEL_OUT_MONO, AudioFormat.ENCODING_PCM_16BIT);
  ```

- 创建 AudioTrack 对象

  AudioTrack 的构造函数为：

  public AudioTrack(int streamType, int sampleRateInHz, int channelConfig, int audioFormat,int bufferSizeInBytes, int mode);

  参数说明:

  streamType：音频类型，和 AudioManager 有关，涉及手机上的音频管理策略。如 STREAM_ALARM（警告声）、STREAM_MUSCI（音乐声）、STREAM_RING（铃声）、STREAM_SYSTEM（系统声音）、STREAM_VOICE_CALL（电话声音）等，将音频分为多种类型的目的是便于对不同类型的音频分别管理。

  sampleRateInHz：采样率;

  channelConfig：声道数;

  audioFormat：采样点位数;

  bufferSizeInBytes：缓冲区大小，自定义的值或利用 getMinBufferSize 得到的值；

  mode：包括 MODE_STATIC 和 MODE_STREAM。其中 MODE_STATIC 表示一次性将所有音频数据放到一个固定地 Buffer 中，然后直接传送给 AudioTrack，适用于数据量小、延时要求高的音频；MODE_STREAM 模式下将会通过 write 分多次将音频数据写到 AudioTrack 中，有一定的延时，这个和我们在 socket 中发送数据一样，应用层从某个地方获取数据。这种方式的坏处就是总是在 JAVA 层和 Native 层交互，效率损失较大。

  使用示例如下：

  ```java
  AduioTrack audioTrack = new AudioTrack(AudioManager.STREAM_MUSIC, 16000, AudioFormat.CHANNEL_OUT_MONO,AudioFormat.ENCODING_PCM_16BIT, bufferSize, AudioTrack.MODE_STREAM);
  ```

- 开始播放

  ```java
  audioTrack.play()；
  ```

- 写入数据

  AudioTrack 使用 write() 方法将数据写入到 Audiotrack 中：

  public int write(byte[] audioData,int offsetInBytes, int sizeInBytes);

  offsetInBytes 是指要播放的数据是从参数 AudioData 的哪个地方开始。

  使用示例如下：

  ```java
  audioTrack.write(buffer, 0, buffer.length);
  ```

- 停止播放

  ```java
  audioTrack.stop();
  audioTrack.release();
  ```



想要更详细并完整地学习 AudioTrack 播放声音的原理及整体代码流程，可以参考知乎文章：[使用AudioTrack进行音频播放](https://www.jianshu.com/p/8a9602e06af6)



## 5 &nbsp;PcmPlayer 类实现音频播放

在本项目中，虽然 SinVoicePlayer 类很重要，但是真正完成声音播放任务的并不是它，而是我们接下来要关注的 PcmPlayer 类。

在实现PcmPlayer 类之前，我们需要注意的几点在于：

（1）PcmPalyer 是通过 AudioTrack 类实现单频率播放的，在初始化 AudioTrack 对象的时候，需要传很多参数，我在代码里面会进行相应的注释。因此，在 SinVoicePlayer 中初始化 PcmPlayer 对象的时候，使用下面的参数进行的初始化：

```java
mPlayer = new PcmPlayer(this, sampleRate, AudioFormat.CHANNEL_OUT_MONO, AudioFormat.ENCODING_PCM_16BIT, bufferSize);
```

这里的 sampleRate 是采样率，默认是 44100Hz，为了保持频率一致，使用单声道的 AudioFormat.CHANNEL_OUT_MONO，同时使用16位的 PCM 格式编码。

（2）为了不断取出要播放的字节数据，需要在 PcmPlayer 类的 start() 方法中使用 while 循环，但值得注意的是，AudioTrack.play() 方法只会执行一次，仅在第一次播放时执行，发出声音。

（3）声音结束播放：在 PcmPlayer 类的 stop() 方法中将 mState 赋值为 STATE_STOP，然后 while 循环退出，从而执行 AudioTrack.stop() 方法，结束声音的播放。

注意到以上几点之后，我们下面来实现 PcmPlayer 声音播放类：

```java
package com.libra.sinvoice;

import android.media.AudioManager;
import android.media.AudioTrack;

import com.libra.sinvoice.Buffer.BufferData;

public class PcmPlayer {
    private final static String TAG = "PcmPlayer";
    private final static int STATE_START = 1;
    private final static int STATE_STOP = 2;
	
    private int mState;//播放状态，播放/停止
    private AudioTrack mAudio;
    private long mPlayedLen;//已经播放过的字节的长度
    private Listener mListener;//PcmListener
    private Callback mCallback;//PcmCallback

    //pcmplayer监听器接口
    public static interface Listener {
        void onPlayStart();
        void onPlayStop();
    }

    public static interface Callback {
        BufferData getPlayBuffer();

        void freePlayData(BufferData data);
    }

    public PcmPlayer(Callback callback, int sampleRate, int channel, int format, int bufferSize) {
        mCallback = callback;
        //初始化AudioTrack对象(音频流类型，采样率，通道，格式，缓冲区大小，模式)
        mAudio = new AudioTrack(AudioManager.STREAM_MUSIC, sampleRate, channel, format, bufferSize, AudioTrack.MODE_STREAM);
        mState = STATE_STOP;
        mPlayedLen = 0;
    }

    public void setListener(Listener listener) {
        mListener = listener;
    }

    public void start() {
        LogHelper.d(TAG, "start");
        if (STATE_STOP == mState && null != mAudio) {
            mPlayedLen = 0;

            if (null != mCallback) {
                mState = STATE_START;
                LogHelper.d(TAG, "start");
                if (null != mListener) {
                    mListener.onPlayStart();
                }
                while (STATE_START == mState) {
                    LogHelper.d(TAG, "start getbuffer");
					//获取要播放的字节数据
                    BufferData data = mCallback.getPlayBuffer();

                    if (null != data) {
                        if (null != data.mData) {
                            int len = mAudio.write(data.mData, 0, data.getFilledSize());//设置要播放的字节数据
							//仅第一次进入，播放声音
                            if (0 == mPlayedLen) {
                                mAudio.play();
                            }
                            mPlayedLen += len;
                            //释放数据
                            mCallback.freePlayData(data);
                        } else {
                            // it is the end of input, so need stop
                            LogHelper.d(TAG, "it is the end of input, so need stop");
                            break;
                        }
                    } else {
                        LogHelper.e(TAG, "get null data");
                        break;
                    }
                }

                if (null != mAudio) {
                    mAudio.pause();
                    mAudio.flush();
                    mAudio.stop();
                }
                mState = STATE_STOP;
                if (null != mListener) {
                    mListener.onPlayStop();
                }
                LogHelper.d(TAG, "end");
            }
        }
    }

    public void stop() {
        if (STATE_START == mState && null != mAudio) {
            mState = STATE_STOP;
        }
    }
}
```



# 下节要做什么？

在本节，我们真正完成了声音的播放任务，它相当于项目中在前方冲锋陷阵的“士兵”，是我们统揽整个项目最直观的感受，也是整个项目方案中离底层最接近的部分。我们真正地把每一个特定频率的音频通过 Andriod 设备的扬声器播放出去，达到了声波通信的目的。

接下来，我们将关注接收端，接收端的大部分代码跟发送端的逻辑相似，因此，相似的逻辑部分便不做解释，实现起来也较为容易。但值得注意的是，接收端在接收时，根据频率得到对应的字符，那么，我们如何从受噪声影响的声音中分辨出声音的具体频率呢？这也是我们项目的一大重点和难点。





